name: ML Model Training

on:
  # Daily schedule (3 AM UTC)
  schedule:
    - cron: '0 3 * * *'
  # Manual trigger
  workflow_dispatch:
    inputs:
      force_retrain:
        description: 'Force retrain even with few samples'
        required: false
        default: 'false'
        type: boolean
      keep_training_data:
        description: 'Keep training data after training (do not archive)'
        required: false
        default: 'false'
        type: boolean

env:
  CARGO_TERM_COLOR: always
  HF_REPO: gagansuie/oxidize-models

jobs:
  train-and-push:
    name: Train Models & Push to HF Hub
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-ml-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-ml-

      - name: Build ML training binary
        run: cargo build --release -p oxidize-common --bin oxidize-train --features ai

      - name: Setup Python (for HF CLI)
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Hugging Face CLI
        run: pip install huggingface_hub --quiet

      - name: Login to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: python scripts/hf_sync.py login

      - name: Download training data from HF Hub
        run: |
          mkdir -p training_data
          mkdir -p training_data_archive
          python scripts/hf_sync.py download
          
          if [ -d "./hf_download/training_data" ]; then
            FILE_COUNT=$(find ./hf_download/training_data -name '*.json' 2>/dev/null | wc -l)
            echo "Found $FILE_COUNT training data files from servers"
            cp -r ./hf_download/training_data/* ./training_data/ 2>/dev/null || true
          fi

      - name: Aggregate training data
        run: |
          python scripts/hf_sync.py aggregate
          echo "Training samples ready: $(find training_data -name '*.json' | wc -l) files"

      - name: Train models
        run: |
          mkdir -p ./trained_models
          
          SAMPLE_COUNT=$(cat training_data/*.json 2>/dev/null | tr -d '\n' | grep -o '"rtt_us"' | wc -l | tr -d ' ')
          SAMPLE_COUNT=${SAMPLE_COUNT:-0}
          echo "Total training samples: $SAMPLE_COUNT"
          
          # Determine if we should use synthetic data
          FORCE_RETRAIN="${{ inputs.force_retrain }}"
          if [ "$SAMPLE_COUNT" -lt 100 ] && [ "$FORCE_RETRAIN" != "true" ]; then
            echo "Using synthetic data (only $SAMPLE_COUNT real samples)"
            SYNTHETIC_FLAG="--generate-synthetic --synthetic-samples 10000"
          else
            echo "Training with $SAMPLE_COUNT real samples"
            SYNTHETIC_FLAG=""
          fi
          
          ./target/release/oxidize-train \
            --input ./training_data \
            --output ./trained_models \
            --transformer-epochs 100 \
            --ppo-steps 1000 \
            $SYNTHETIC_FLAG \
            --verbose

      - name: Prepare HF repo upload
        run: |
          # Copy trained models
          cp ./trained_models/*.safetensors ./hf_repo/ 2>/dev/null || true
          cp ./trained_models/config.json ./hf_repo/config.json 2>/dev/null || true
          
          # Archive old training data (move to archive folder)
          KEEP_DATA="${{ inputs.keep_training_data }}"
          if [ "$KEEP_DATA" != "true" ]; then
            mkdir -p ./hf_repo/training_data_archive
            TIMESTAMP=$(date +%Y%m%d_%H%M%S)
            
            # Move processed training data to archive
            if [ -d "./hf_download/training_data" ]; then
              for f in ./hf_download/training_data/training-*.json; do
                if [ -f "$f" ]; then
                  BASENAME=$(basename "$f")
                  mv "$f" "./hf_repo/training_data_archive/${TIMESTAMP}_${BASENAME}" 2>/dev/null || true
                fi
              done
            fi
            
            # Clear the training_data folder (will be repopulated by servers)
            rm -rf ./hf_repo/training_data/*.json 2>/dev/null || true
            mkdir -p ./hf_repo/training_data
            echo "Training data archived and cleared for next cycle"
          fi

      - name: Push models to HF Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: python scripts/hf_sync.py upload

      - name: Summary
        run: |
          SAMPLE_COUNT=$(cat training_data/*.json 2>/dev/null | tr -d '\n' | grep -o '"rtt_us"' | wc -l | tr -d ' ')
          SAMPLE_COUNT=${SAMPLE_COUNT:-0}
          echo "## ML Training Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Training samples aggregated**: $SAMPLE_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Models pushed to**: https://huggingface.co/$HF_REPO" >> $GITHUB_STEP_SUMMARY
          echo "- **Timestamp**: $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Pipeline Status" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Servers auto-collect training data" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Servers auto-upload to HF Hub (hourly)" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ CI aggregates and trains (daily)" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Servers auto-download new models (on startup)" >> $GITHUB_STEP_SUMMARY
